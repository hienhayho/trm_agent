# TRM Default Configuration
# Based on the paper "Less is More: Recursive Reasoning with Tiny Networks"
# Note: Span extraction (slots/params) is handled by GLiNER2, not TRM.

model:
  # Architecture
  hidden_size: 512
  num_layers: 2  # Paper finding: 2 layers is optimal
  num_heads: 8
  # intermediate_size: 2048
  intermediate_size: 1024 
  dropout: 0.1
  use_attention: true  # Set false for MLP-Mixer (better for small fixed context)

  # Vocabulary
  vocab_size: 32000
  max_seq_len: 1024
  pad_token_id: 0

  # TRM specific
  use_trm_loop: false     # Use TRM recursive loop (false = single pass, let Mamba handle recurrence)
  n_latent_recursion: 6  # n: latent reasoning iterations
  T_deep_recursion: 3    # T: deep recursion iterations
  N_supervision: 2       # N_sup: max supervision steps

  # URM innovations (from Universal Reasoning Model paper)
  use_conv_swiglu: true   # Use ConvSwiGLU instead of SwiGLU (+5-8% accuracy)
  conv_kernel_size: 2     # Short conv kernel size for local feature mixing
  tbptl_no_grad_steps: 0  # Skip loss on first N supervision steps (TBPTL)

  # Hybrid architecture (Mamba + MoE + Attention)
  use_hybrid_block: false  # Enable hybrid blocks (requires mamba-ssm)

  # Mamba configuration (only used when use_hybrid_block=true)
  mamba_version: 2        # Mamba version: 1 or 2 (default: 2 for better performance)
  mamba_d_state: 16       # SSM state dimension (Mamba1 only)
  mamba_d_conv: 4         # Local convolution width
  mamba_expand: 2         # Block expansion factor
  mamba_headdim: 64       # Head dimension (Mamba2 only)
  mamba_chunk_size: 256   # Chunk size for Mamba2 (must be power of 2)
  mamba_use_mem_eff_path: true  # Use optimized CUDA kernels (set false if training hangs)

  # MoE configuration (only used when use_hybrid_block=true)
  num_experts: 4          # Number of expert MLPs
  num_experts_per_tok: 2  # Top-k experts per token
  moe_intermediate_size: 1024  # Expert MLP hidden size

  # Output dimensions
  num_tools: 10

  # Role tokens
  num_roles: 4

  # Training
  ema_decay: 0.999

  # Loss weights (TRM only handles decision + tool classification)
  decision_loss_weight: 1.0
  tool_loss_weight: 1.0
  q_loss_weight: 0.5

  # Focal Loss (for imbalanced decision classification)
  focal_alpha: 0.25
  focal_gamma: 1.0

training:
  # Optimizer (from paper)
  learning_rate: 1.0e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  max_grad_norm: 1.0

  # Scheduler
  warmup_steps: 2000
  num_epochs: 100

  # Training
  batch_size: 1  # Reduced for memory (use gradient_accumulation to simulate larger batch)
  gradient_accumulation_steps: 4  # Effective batch size = 8 * 4 = 32

  # Memory optimization
  use_amp: true  # Automatic Mixed Precision - reduces memory ~50%
  amp_dtype: float16  # or bfloat16 for newer GPUs

  # EMA
  use_ema: true
  ema_decay: 0.999

  # ACT
  use_act: true

  # Logging
  log_interval: 100
  eval_interval: 500
  save_interval: 1000

  # Paths
  output_dir: outputs
