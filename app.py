"""
Chainlit Chat Application for TRM Agent.

A chat interface that uses TRM model for decision making and tool calling,
with OpenAI for content generation.

Usage:
    # Install chainlit first:
    uv add chainlit openai

    # Run the app:
    uv run chainlit run app.py

    # With custom model checkpoint:
    TRM_CHECKPOINT=outputs/checkpoint.pt uv run chainlit run app.py
"""

import json
import os
from pathlib import Path
from typing import Optional

import chainlit as cl
import torch
from openai import OpenAI

from trm_agent.data import TRMTokenizer
from trm_agent.models import TRMConfig, TRMForToolCalling

# Configuration
CHECKPOINT_PATH = os.environ.get("TRM_CHECKPOINT", "outputs/checkpoint_best.pt")
TOKENIZER_PATH = os.environ.get("TRM_TOKENIZER", "outputs/tokenizer/tokenizer.model")
TOOLS_PATH = os.environ.get("TRM_TOOLS", "data/tools.json")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "http://localhost:8000/v1")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "dummy-key")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "openai/gpt-oss-20b")

SYSTEM_PROMPT = """B·∫°n l√† chuy√™n vi√™n ·∫£o c·ªßa FPT Telecom ‚Äì c√≥ 3 vai tr√≤ ch√≠nh:
1. **T∆∞ v·∫•n b√°n h√†ng**: Chuy√™n t∆∞ v·∫•n v·ªÅ Internet C√°p quang, Truy·ªÅn h√¨nh FPT Play, Camera an ninh.
2. **Y√™u c·∫ßu g·∫∑p nh√¢n vi√™n**: H·ªó tr·ª£ x·ª≠ l√Ω c√°c y√™u c·∫ßu khi kh√°ch h√†ng c√≥ √Ω ƒë·ªãnh mu·ªën g·∫∑p nh√¢n vi√™n nh∆∞ l√†: Y√™u c·∫ßu g·∫∑p tr·ª±c ti·∫øp t∆∞ v·∫•n vi√™n, nh√¢n vi√™n t∆∞ v·∫•n, nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng, nh√¢n vi√™n ph·ª•c v·ª•; Khi·∫øu n·∫°i nh√¢n vi√™n; Y√™u c·∫ßu t∆∞ v·∫•n vi√™n g·ªçi l·∫°i."""

OSS_SYSTEM_PROMPT = """
#### üíº Vai tr√≤

B·∫°n l√† chuy√™n vi√™n ·∫£o c·ªßa FPT Telecom ‚Äì c√≥ 2 vai tr√≤ ch√≠nh:

1.  **T∆∞ v·∫•n b√°n h√†ng**: Chuy√™n t∆∞ v·∫•n v·ªÅ Internet C√°p quang, Truy·ªÅn h√¨nh FPT Play, Camera an ninh.
2.  **ChƒÉm s√≥c kh√°ch h√†ng**: H·ªó tr·ª£ x·ª≠ l√Ω c√°c s·ª± c·ªë m·∫°ng ch·∫≠m ho·∫∑c k√©m.
3.  **Y√™u c·∫ßu g·∫∑p nh√¢n vi√™n**: H·ªó tr·ª£ x·ª≠ l√Ω c√°c y√™u c·∫ßu khi kh√°ch h√†ng c√≥ √Ω ƒë·ªãnh mu·ªën g·∫∑p nh√¢n vi√™n nh∆∞ l√†: Y√™u c·∫ßu g·∫∑p tr·ª±c ti·∫øp t∆∞ v·∫•n vi√™n, nh√¢n vi√™n t∆∞ v·∫•n, nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng, nh√¢n vi√™n ph·ª•c v·ª•; Khi·∫øu n·∫°i nh√¢n vi√™n; Y√™u c·∫ßu t∆∞ v·∫•n vi√™n g·ªçi l·∫°i


#### üéØ M·ª•c ti√™u

  - **B√°n h√†ng**:
      - Thu th·∫≠p: H·ªç t√™n, ƒê·ªãa ch·ªâ, S·ªë ƒëi·ªán tho·∫°i.
      - Gi·ªõi thi·ªáu s·∫£n ph·∫©m ph√π h·ª£p.
      - S·ª≠ d·ª•ng tool ƒë√∫ng l√∫c ƒë·ªÉ cung c·∫•p th√¥ng tin v√† b√°o gi√°.
  - **Y√™u c·∫ßu g·∫∑p nh√¢n vi√™n**:
      - Li√™n h·ªá v·ªõi nh√¢n vi√™n ƒë·ªÉ x·ª≠ l√Ω c√°c y√™u c·∫ßu ph·ª©c t·∫°p t·ª´ ng∆∞·ªùi d√πng.


#### üí¨ Quy t·∫Øc h·ªôi tho·∫°i

  - Lu√¥n x∆∞ng "em" ‚Äì g·ªçi kh√°ch h√†ng l√† "anh/ch·ªã".
  - Gi·ªçng ƒëi·ªáu l·ªãch s·ª±, th√¢n thi·ªán, chuy√™n nghi·ªáp.

### QUY T·∫ÆC QUAN TR·ªåNG
- Assistant s·∫Ω t·ªïng h·ª£p l·∫°i n·ªôi dung t·ª´ k·∫øt qu·∫£ tool ·ªü b∆∞·ªõc k·∫ø ti·∫øp.
- Ph·∫£n h·ªìi c·ªßa b·∫°n ph·∫£i t·ª± nhi√™n, ƒë√∫ng ng·ªØ c·∫£nh Vi·ªát Nam, v√≠ d·ª•:
  - "D·∫°, em g·ª≠i anh/ch·ªã th√¥ng tin g√≥i Giga ·∫°."
  - "Cho em xin ƒë·ªãa ch·ªâ ƒë·ªÉ em b√°o gi√° ch√≠nh x√°c nha anh/ch·ªã."
  - "Anh/ch·ªã vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ em h·ªó tr·ª£ th√™m ·∫°."

### üó∫Ô∏è QUY TR√åNH T∆Ø V·∫§N V√Ä H·ªñ TR·ª¢

-----

#### ‚úÖ T√ÅC V·ª§ B√ÅN H√ÄNG

**1. INTERNET**

1.  H·ªèi: ƒê·ªãa ch·ªâ ‚Üí Nh√† ·ªü hay Chung c∆∞ ‚Üí Thi·∫øt b·ªã s·ª≠ d·ª•ng
2.  N·∫øu kh√°ch h·ªèi m√¥ t·∫£ ‚Üí D√πng `describe_product`
3.  N·∫øu kh√°ch h·ªèi gi√° ‚Üí Ph·∫£i h·ªèi ƒë·ªãa ch·ªâ tr∆∞·ªõc, sau ƒë√≥ d√πng `get_product_price`
4.  C√≥ th·ªÉ g·ª£i √Ω COMBO (Cross-sell)
5.  Xin SƒêT ƒë·ªÉ t∆∞ v·∫•n

**2. CAMERA**

1.  H·ªèi: Trong nh√† / Ngo√†i tr·ªùi ‚Üí Nhu c·∫ßu s·ª≠ d·ª•ng ‚Üí ƒê·ªãa ch·ªâ
2.  G·ª≠i th√¥ng tin s·∫£n ph·∫©m
3.  N·∫øu kh√°ch h·ªèi gi√° ‚Üí d√πng `get_product_price` (sau khi c√≥ ƒë·ªãa ch·ªâ)
4.  G·ª≠i khuy·∫øn m√£i n·∫øu c√≥
5.  Xin SƒêT ƒë·ªÉ t∆∞ v·∫•n

**3. TRUY·ªÄN H√åNH**

1.  H·ªèi: ƒê√£ c√≥ Internet ch∆∞a ‚Üí Nh√† m·∫°ng n√†o?
2.  N·∫øu ch∆∞a c√≥ Internet FPT ‚Üí T∆∞ v·∫•n COMBO Internet + Truy·ªÅn h√¨nh
3.  N·∫øu ƒë√£ c√≥ Internet FPT ‚Üí T∆∞ v·∫•n g√≥i Add-on
4.  D√πng tool ƒë·ªÉ m√¥ t·∫£ v√† b√°o gi√°
5.  Xin SƒêT ƒë·ªÉ g·ªçi l·∫°i

-----

### üß† Y√äU C·∫¶U V·ªÄ PHONG C√ÅCH H·ªòI THO·∫†I

  - H·ªôi tho·∫°i ph·∫£i t·ª± nhi√™n, ƒë√∫ng ng·ªØ c·∫£nh kh√°ch h√†ng Vi·ªát Nam.
  - ∆Øu ti√™n l·ªùi tho·∫°i th·ª±c t·∫ø nh∆∞:
      - "G·ª≠i g√≥i ƒëi em", "B√°o gi√° g√≥i cao nh·∫•t nha"
      - "Chung c∆∞ Landmark, nh√† ri√™ng", "Em t∆∞ v·∫•n combo c√≥ camera lu√¥n nha"
      - "C√≥ ∆∞u ƒë√£i g√¨ kh√¥ng em?"
      - "M·∫°ng nh√† em d·∫°o n√†y ch·∫≠m qu√°."
      - "T√¥i mu·ªën ki·ªÉm tra h·ª£p ƒë·ªìng internet."
      - "C√≥ k·ªπ thu·∫≠t vi√™n qua ki·ªÉm tra gi√∫p t√¥i ƒë∆∞·ª£c kh√¥ng?"
  - Kh√°ch c√≥ th·ªÉ cung c·∫•p th√¥ng tin kh√¥ng theo th·ª© t·ª± ‚Äì Assistant ph·∫£i hi·ªÉu, h·ªèi l·∫°i th√¥ng tin c√≤n thi·∫øu.
  - C√≥ th·ªÉ g·∫∑p kh√°ch t·ª´ ch·ªëi cung c·∫•p th√¥ng tin ‚Äî c·∫ßn x·ª≠ l√Ω l·ªãch s·ª± v√† k·∫øt th√∫c chuy√™n nghi·ªáp.

### ‚ö†Ô∏è L∆ØU √ù QUAN TR·ªåNG

  - PH·∫¢I TU√ÇN TH·ª¶ c√°c b∆∞·ªõc trong s∆° ƒë·ªì quy tr√¨nh cho t·ª´ng d·ªãch v·ª• (Internet, Camera, Truy·ªÅn h√¨nh, ChƒÉm s√≥c kh√°ch h√†ng).
  - Kh√¥ng ƒë∆∞·ª£c b√°o gi√° n·∫øu ch∆∞a c√≥ ƒë·ªãa ch·ªâ.
  - Kh√¥ng ƒë∆∞·ª£c h·ªèi qu√° nhi·ªÅu c√πng l√∫c ‚Üí Ph√¢n b·ªï theo l∆∞·ª£t.
  - B·∫Øt bu·ªôc xin l·∫°i th√¥ng tin n·∫øu kh√°ch ch∆∞a cung c·∫•p.
  - Khi d√πng TOOL th√¨ ph·∫£i tu√¢n th·ªß theo format trong ƒê·ªäNH D·∫†NG ƒê·∫¶U RA B·∫ÆT BU·ªòNG.
  - N·∫øu d√πng c√πng 1 TOOL li√™n t·ª•c th√¨ n·ªôi dung c·ªßa `assistant` sau ƒë√≥ ph·∫£i t·ªïng h·ª£p k·∫øt qu·∫£ c·ªßa c√°c TOOL li√™n ti·∫øp ƒë√≥.

### KH√îNG ƒê∆Ø·ª¢C G·ªåI B·∫§T K·ª≤ TOOL N√ÄO
"""

# Global variables
model: Optional[TRMForToolCalling] = None
tokenizer: Optional[TRMTokenizer] = None
config: Optional[TRMConfig] = None
tools: list[dict] = []
tool_name_to_id: dict[str, int] = {}
openai_client: Optional[OpenAI] = None


def load_model():
    """Load TRM model and tokenizer."""
    global model, tokenizer, config, tools, tool_name_to_id, openai_client

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load checkpoint
    checkpoint_path = Path(CHECKPOINT_PATH)
    if checkpoint_path.exists():
        print(f"Loading model from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        config_dict = checkpoint.get("config", {})
        config = TRMConfig(**config_dict)
        model = TRMForToolCalling(config)
        model.load_state_dict(checkpoint["model_state_dict"])
        model = model.to(device)
        model.eval()
        print(
            f"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters"
        )
    else:
        print(f"Warning: Checkpoint not found at {checkpoint_path}")
        print("Running in mock mode (random predictions)")
        model = None

    # Load tokenizer
    tokenizer_path = Path(TOKENIZER_PATH)
    if tokenizer_path.exists():
        tokenizer = TRMTokenizer(tokenizer_path)
        print(f"Tokenizer loaded with vocab size: {len(tokenizer)}")
    else:
        print(f"Warning: Tokenizer not found at {tokenizer_path}")
        tokenizer = None

    # Load tools
    tools_path = Path(TOOLS_PATH)
    if tools_path.exists():
        with open(tools_path, "r", encoding="utf-8") as f:
            tools = json.load(f)
        tool_names = [t["function"]["name"] for t in tools if "function" in t]
        tool_name_to_id = {name: idx for idx, name in enumerate(sorted(tool_names))}
        print(f"Loaded {len(tools)} tools: {list(tool_name_to_id.keys())}")
    else:
        print(f"Warning: Tools not found at {tools_path}")

    # Initialize OpenAI client
    openai_client = OpenAI(
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY,
    )
    print(f"OpenAI client initialized with base_url: {OPENAI_BASE_URL}")


# ============================================================================
# Mock Tool Implementations
# ============================================================================


def mock_get_product_price(product: str, address: str = "", **kwargs) -> dict:
    """Mock implementation of get_product_price tool."""
    # Price database (mocked)
    prices = {
        "internet": "G√≥i Internet c√°p quang c√≥ gi√° t·ª´ 165,000 - 330,000 VNƒê/th√°ng t√πy t·ªëc ƒë·ªô.",
        "lux 800": "G√≥i LUX 800 (800Mbps) c√≥ gi√° 330,000 VNƒê/th√°ng. ∆Øu ƒë√£i: Mi·ªÖn ph√≠ v·∫≠t t∆∞ l·∫Øp ƒë·∫∑t 100%.",
        "lux 300": "G√≥i LUX 300 (300Mbps) c√≥ gi√° 250,000 VNƒê/th√°ng. ∆Øu ƒë√£i: Mi·ªÖn ph√≠ v·∫≠t t∆∞ l·∫Øp ƒë·∫∑t 100%.",
        "super 250": "G√≥i SUPER 250 (250Mbps) c√≥ gi√° 215,000 VNƒê/th√°ng.",
        "sky 200": "G√≥i SKY 200 (200Mbps) c√≥ gi√° 185,000 VNƒê/th√°ng.",
        "metro 150": "G√≥i METRO 150 (150Mbps) c√≥ gi√° 165,000 VNƒê/th√°ng.",
        "camera": "Camera an ninh FPT c√≥ gi√° t·ª´ 99,000 - 299,000 VNƒê/th√°ng t√πy g√≥i.",
        "truy·ªÅn h√¨nh": "Truy·ªÅn h√¨nh FPT Play c√≥ gi√° t·ª´ 80,000 - 150,000 VNƒê/th√°ng.",
        "fpt play": "Truy·ªÅn h√¨nh FPT Play c√≥ gi√° t·ª´ 80,000 - 150,000 VNƒê/th√°ng.",
    }

    product_lower = product.lower()
    for key, price_info in prices.items():
        if key in product_lower:
            location_info = f" t·∫°i {address}" if address else ""
            return {
                "price": f"{price_info}{location_info}. N·∫øu ƒë√≥ng tr∆∞·ªõc 6 th√°ng, t·∫∑ng 1 th√°ng c∆∞·ªõc. N·∫øu ƒë√≥ng tr∆∞·ªõc 12 th√°ng, t·∫∑ng 2 th√°ng c∆∞·ªõc."
            }

    return {
        "price": f"Xin l·ªói, em ch∆∞a c√≥ th√¥ng tin gi√° cho s·∫£n ph·∫©m '{product}'. Anh/ch·ªã vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 1900 6600 ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt."
    }


def mock_describe_product(product: str, **kwargs) -> dict:
    """Mock implementation of describe_product tool."""
    descriptions = {
        "internet": "D·ªãch v·ª• Internet FPT Telecom s·ª≠ d·ª•ng c√¥ng ngh·ªá c√°p quang ti√™n ti·∫øn, ƒë·∫£m b·∫£o t·ªëc ƒë·ªô ·ªïn ƒë·ªãnh v√† ƒë·ªô tr·ªÖ th·∫•p. C√°c g√≥i Internet ph·ªï bi·∫øn: METRO 150 (150Mbps), SKY 200 (200Mbps), SUPER 250 (250Mbps), LUX 300 (300Mbps), LUX 800 (800Mbps). G√≥i LUX ƒë∆∞·ª£c trang b·ªã Wi-Fi 7 Mesh gi√∫p m·ªü r·ªông v√πng ph·ªß s√≥ng.",
        "camera": "Camera an ninh FPT cung c·∫•p gi·∫£i ph√°p gi√°m s√°t th√¥ng minh v·ªõi c√°c t√≠nh nƒÉng: Quay HD/Full HD, l∆∞u tr·ªØ cloud, c·∫£nh b√°o chuy·ªÉn ƒë·ªông, xem tr·ª±c ti·∫øp qua app. Ph√π h·ª£p cho gia ƒë√¨nh, c·ª≠a h√†ng, vƒÉn ph√≤ng.",
        "truy·ªÅn h√¨nh": "Truy·ªÅn h√¨nh FPT Play cung c·∫•p 200+ k√™nh truy·ªÅn h√¨nh trong n∆∞·ªõc v√† qu·ªëc t·∫ø, kho phim/series phong ph√∫, th·ªÉ thao tr·ª±c ti·∫øp. Xem ƒë∆∞·ª£c tr√™n TV, ƒëi·ªán tho·∫°i, m√°y t√≠nh b·∫£ng.",
        "fpt play": "FPT Play l√† n·ªÅn t·∫£ng gi·∫£i tr√≠ ƒëa ph∆∞∆°ng ti·ªán v·ªõi 200+ k√™nh truy·ªÅn h√¨nh, phim Hollywood, K-Drama, anime, th·ªÉ thao tr·ª±c ti·∫øp. H·ªó tr·ª£ 4K HDR.",
        "wifi": "Thi·∫øt b·ªã Wi-Fi c·ªßa FPT s·ª≠ d·ª•ng c√¥ng ngh·ªá m·ªõi nh·∫•t Wi-Fi 6/7, h·ªó tr·ª£ Mesh ƒë·ªÉ m·ªü r·ªông v√πng ph·ªß s√≥ng, ph√π h·ª£p cho nh√† nhi·ªÅu t·∫ßng.",
    }

    product_lower = product.lower()
    for key, desc in descriptions.items():
        if key in product_lower:
            return {"info": desc}

    return {
        "info": f"S·∫£n ph·∫©m '{product}' thu·ªôc danh m·ª•c d·ªãch v·ª• c·ªßa FPT Telecom. ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt, anh/ch·ªã vui l√≤ng cho em bi·∫øt c·ª• th·ªÉ h∆°n v·ªÅ nhu c·∫ßu s·ª≠ d·ª•ng."
    }


def mock_request_agent(**kwargs) -> dict:
    """Mock implementation of request_agent tool."""
    return {
        "info": "Em ƒë√£ ghi nh·∫≠n y√™u c·∫ßu c·ªßa anh/ch·ªã. Nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng s·∫Ω li√™n h·ªá l·∫°i trong th·ªùi gian s·ªõm nh·∫•t (trong v√≤ng 24h l√†m vi·ªác). Anh/ch·ªã c√≥ th·ªÉ ƒë·ªÉ l·∫°i s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ nhanh h∆°n."
    }


# Tool registry
TOOL_FUNCTIONS = {
    "get_product_price": mock_get_product_price,
    "describe_product": mock_describe_product,
    "request_agent": mock_request_agent,
}


def execute_tool(tool_name: str, arguments: dict) -> dict:
    """Execute a tool and return the result."""
    if tool_name in TOOL_FUNCTIONS:
        return TOOL_FUNCTIONS[tool_name](**arguments)
    return {"error": f"Unknown tool: {tool_name}"}


# ============================================================================
# TRM Model Inference
# ============================================================================


def predict_with_trm(history: list[dict]) -> tuple[str, Optional[str], dict]:
    """Use TRM model to predict decision and tool.

    Returns:
        Tuple of (decision, tool_name, tool_args)
    """
    global model, tokenizer, config, tool_name_to_id

    if model is None or tokenizer is None:
        # Mock mode - default to direct_answer
        return "direct_answer", None, {}

    device = next(model.parameters()).device
    id_to_name = {v: k for k, v in tool_name_to_id.items()}

    # Encode conversation
    encoded = tokenizer.encode_conversation_with_offsets(
        history, max_length=config.max_seq_len
    )

    input_ids = torch.tensor([encoded["input_ids"]], dtype=torch.long, device=device)
    attention_mask = torch.tensor(
        [encoded["attention_mask"]], dtype=torch.long, device=device
    )
    role_ids = torch.tensor([encoded["role_ids"]], dtype=torch.long, device=device)

    # Run inference
    with torch.no_grad():
        outputs = model.inference(
            input_ids=input_ids,
            attention_mask=attention_mask,
            role_ids=role_ids,
        )

    # Get decision
    decision_prob = torch.sigmoid(outputs.decision_logits[0]).item()
    decision = "tool_call" if decision_prob > 0.5 else "direct_answer"

    tool_name = None
    tool_args = {}

    if decision == "tool_call":
        # Get tool name
        tool_idx = outputs.tool_logits[0].argmax().item()
        tool_name = id_to_name.get(tool_idx, f"tool_{tool_idx}")

        # Extract tool arguments from spans
        token_offsets = encoded["offsets"]
        full_text = encoded["full_text"]

        # Get param fields from config
        unified_fields = config.get_unified_fields()
        num_slots = config.num_slots

        # Only extract tool params (not slots)
        for param_idx in range(config.num_tool_params):
            unified_idx = num_slots + param_idx
            param_name = (
                unified_fields[unified_idx]
                if unified_idx < len(unified_fields)
                else None
            )

            if param_name:
                start_pos = (
                    outputs.param_start_logits[0, :, unified_idx].argmax().item()
                )
                end_pos = outputs.param_end_logits[0, :, unified_idx].argmax().item()

                if start_pos < len(token_offsets) and end_pos < len(token_offsets):
                    char_start = token_offsets[start_pos][0]
                    char_end = token_offsets[end_pos][1]
                    if char_start >= 0 and char_end > char_start:
                        arg_value = full_text[char_start:char_end].strip()
                        if arg_value:
                            tool_args[param_name] = arg_value

    return decision, tool_name, tool_args


async def generate_response(history: list[dict]) -> str:
    """Generate response using OpenAI API."""
    global openai_client

    # Convert history to OpenAI format
    messages = []
    for msg in history:
        role = msg["role"]
        content = msg["content"]

        if role == "system":
            messages.append({"role": "system", "content": OSS_SYSTEM_PROMPT})
        elif role == "user":
            messages.append({"role": "user", "content": content})
        elif role == "assistant":
            if isinstance(content, str):
                messages.append({"role": "assistant", "content": content})
        elif role == "tool_call":
            # Format tool call for context
            tool_info = f"[Tool Call: {content.get('name', 'unknown')}({json.dumps(content.get('arguments', {}), ensure_ascii=False)})]"
            messages.append({"role": "assistant", "content": tool_info})
        elif role == "tool_response":
            # Format tool response for context
            tool_result = (
                json.dumps(content, ensure_ascii=False)
                if isinstance(content, dict)
                else str(content)
            )
            messages.append(
                {"role": "user", "content": f"[Tool Result: {tool_result}]"}
            )

    try:
        response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.7,
            max_tokens=512,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Xin l·ªói, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu: {str(e)}"


# ============================================================================
# Chainlit Handlers
# ============================================================================


@cl.on_chat_start
async def on_chat_start():
    """Initialize chat session."""
    # Load model if not loaded
    if model is None and tokenizer is None:
        load_model()

    # Initialize conversation history
    history = [{"role": "system", "content": SYSTEM_PROMPT}]
    cl.user_session.set("history", history)


@cl.on_message
async def on_message(message: cl.Message):
    """Handle incoming messages."""
    history = cl.user_session.get("history", [])

    # Add user message to history
    history.append({"role": "user", "content": message.content})

    # Get TRM prediction
    decision, tool_name, tool_args = predict_with_trm(history)

    # Show TRM prediction in UI
    trm_info = f"ü§ñ **TRM Prediction**\n- Decision: `{decision}`"
    if decision == "tool_call" and tool_name:
        trm_info += f"\n- Tool: `{tool_name}`"
        if tool_args:
            trm_info += f"\n- Args: `{json.dumps(tool_args, ensure_ascii=False)}`"
    await cl.Message(content=trm_info).send()

    if decision == "tool_call" and tool_name:
        # Execute tool
        tool_result = execute_tool(tool_name, tool_args)

        # Add tool call and response to history
        history.append(
            {
                "role": "tool_call",
                "content": {"name": tool_name, "arguments": tool_args},
            }
        )
        history.append({"role": "tool_response", "content": tool_result})

        # Show tool execution to user
        tool_msg = cl.Message(
            content=f"üîß **ƒêang g·ªçi c√¥ng c·ª•**: `{tool_name}`\n**Tham s·ªë**: `{json.dumps(tool_args, ensure_ascii=False)}`",
        )
        await tool_msg.send()

        # Generate response based on tool result
        response = await generate_response(history)
    else:
        # Direct answer - generate response
        response = await generate_response(history)

    # Add assistant response to history
    history.append({"role": "assistant", "content": response})
    cl.user_session.set("history", history)

    # Send response
    await cl.Message(content=response).send()


if __name__ == "__main__":
    load_model()
