"""
Chainlit Chat Application for TRM Agent.

A chat interface that uses TRM model for decision making and tool calling,
GLiNER2 for entity extraction (slots and tool arguments),
with OpenAI for content generation.

Usage:
    # Install chainlit first:
    uv add chainlit openai

    # Run the app:
    uv run chainlit run app.py

    # With custom TRM model checkpoint:
    TRM_CHECKPOINT=outputs/checkpoint.pt uv run chainlit run app.py

    # With custom GLiNER2 LoRA adapter (after fine-tuning):
    GLINER2_ADAPTER=outputs/gliner2/final uv run chainlit run app.py

    # With both custom models:
    TRM_CHECKPOINT=outputs/checkpoint.pt \\
    GLINER2_ADAPTER=outputs/gliner2/final \\
    uv run chainlit run app.py

Environment Variables:
    TRM_CHECKPOINT: Path to TRM model checkpoint
    TRM_TOKENIZER: Path to TRM tokenizer
    TRM_TOOLS: Path to tools.json
    GLINER2_MODEL: Base GLiNER2 model (default: fastino/gliner2-multi-v1)
    GLINER2_ADAPTER: Path to LoRA adapter directory (optional)
    GLINER2_THRESHOLD: Entity extraction threshold (default: 0.5)
    OPENAI_BASE_URL: OpenAI API base URL
    OPENAI_API_KEY: OpenAI API key
    OPENAI_MODEL: OpenAI model name
"""

import json
import os
import re
from pathlib import Path
from typing import Optional

import chainlit as cl
import torch
from openai import OpenAI

from trm_agent.data import TRMTokenizer
from trm_agent.inference import GLiNER2Extractor
from trm_agent.models import TRMConfig, TRMForToolCalling

# Configuration
CHECKPOINT_PATH = os.environ.get("TRM_CHECKPOINT", "outputs/checkpoint_best.pt")
TOKENIZER_PATH = os.environ.get("TRM_TOKENIZER", "outputs/tokenizer/tokenizer.model")
TOOLS_PATH = os.environ.get("TRM_TOOLS", "data/tools.json")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "http://localhost:8000/v1")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "dummy-key")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "openai/gpt-oss-20b")

# GLiNER2 Configuration
GLINER2_MODEL = os.environ.get("GLINER2_MODEL", "fastino/gliner2-multi-v1")
GLINER2_ADAPTER = os.environ.get("GLINER2_ADAPTER", "")  # Path to LoRA adapter
GLINER2_THRESHOLD = float(os.environ.get("GLINER2_THRESHOLD", "0.5"))

SYSTEM_PROMPT = """Báº¡n lÃ  chuyÃªn viÃªn áº£o cá»§a FPT Telecom â€“ cÃ³ 3 vai trÃ² chÃ­nh:
1. **TÆ° váº¥n bÃ¡n hÃ ng**: ChuyÃªn tÆ° váº¥n vá» Internet CÃ¡p quang, Truyá»n hÃ¬nh FPT Play, Camera an ninh.
2. **YÃªu cáº§u gáº·p nhÃ¢n viÃªn**: Há»— trá»£ xá»­ lÃ½ cÃ¡c yÃªu cáº§u khi khÃ¡ch hÃ ng cÃ³ Ã½ Ä‘á»‹nh muá»‘n gáº·p nhÃ¢n viÃªn nhÆ° lÃ : YÃªu cáº§u gáº·p trá»±c tiáº¿p tÆ° váº¥n viÃªn, nhÃ¢n viÃªn tÆ° váº¥n, nhÃ¢n viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng, nhÃ¢n viÃªn phá»¥c vá»¥; Khiáº¿u náº¡i nhÃ¢n viÃªn; YÃªu cáº§u tÆ° váº¥n viÃªn gá»i láº¡i."""

OSS_SYSTEM_PROMPT = """
#### ðŸ’¼ Vai trÃ²

Báº¡n lÃ  chuyÃªn viÃªn áº£o cá»§a FPT Telecom â€“ cÃ³ 2 vai trÃ² chÃ­nh:

1.  **TÆ° váº¥n bÃ¡n hÃ ng**: ChuyÃªn tÆ° váº¥n vá» Internet CÃ¡p quang, Truyá»n hÃ¬nh FPT Play, Camera an ninh.
2.  **ChÄƒm sÃ³c khÃ¡ch hÃ ng**: Há»— trá»£ xá»­ lÃ½ cÃ¡c sá»± cá»‘ máº¡ng cháº­m hoáº·c kÃ©m.
3.  **YÃªu cáº§u gáº·p nhÃ¢n viÃªn**: Há»— trá»£ xá»­ lÃ½ cÃ¡c yÃªu cáº§u khi khÃ¡ch hÃ ng cÃ³ Ã½ Ä‘á»‹nh muá»‘n gáº·p nhÃ¢n viÃªn nhÆ° lÃ : YÃªu cáº§u gáº·p trá»±c tiáº¿p tÆ° váº¥n viÃªn, nhÃ¢n viÃªn tÆ° váº¥n, nhÃ¢n viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng, nhÃ¢n viÃªn phá»¥c vá»¥; Khiáº¿u náº¡i nhÃ¢n viÃªn; YÃªu cáº§u tÆ° váº¥n viÃªn gá»i láº¡i


#### ðŸŽ¯ Má»¥c tiÃªu

  - **BÃ¡n hÃ ng**:
      - Thu tháº­p: Há» tÃªn, Äá»‹a chá»‰, Sá»‘ Ä‘iá»‡n thoáº¡i.
      - Giá»›i thiá»‡u sáº£n pháº©m phÃ¹ há»£p.
      - Sá»­ dá»¥ng tool Ä‘Ãºng lÃºc Ä‘á»ƒ cung cáº¥p thÃ´ng tin vÃ  bÃ¡o giÃ¡.
  - **YÃªu cáº§u gáº·p nhÃ¢n viÃªn**:
      - LiÃªn há»‡ vá»›i nhÃ¢n viÃªn Ä‘á»ƒ xá»­ lÃ½ cÃ¡c yÃªu cáº§u phá»©c táº¡p tá»« ngÆ°á»i dÃ¹ng.


#### ðŸ’¬ Quy táº¯c há»™i thoáº¡i

  - LuÃ´n xÆ°ng "em" â€“ gá»i khÃ¡ch hÃ ng lÃ  "anh/chá»‹".
  - Giá»ng Ä‘iá»‡u lá»‹ch sá»±, thÃ¢n thiá»‡n, chuyÃªn nghiá»‡p.

### QUY Táº®C QUAN TRá»ŒNG
- Assistant sáº½ tá»•ng há»£p láº¡i ná»™i dung tá»« káº¿t quáº£ tool á»Ÿ bÆ°á»›c káº¿ tiáº¿p.
- Pháº£n há»“i cá»§a báº¡n pháº£i tá»± nhiÃªn, Ä‘Ãºng ngá»¯ cáº£nh Viá»‡t Nam, vÃ­ dá»¥:
  - "Dáº¡, em gá»­i anh/chá»‹ thÃ´ng tin gÃ³i Giga áº¡."
  - "Cho em xin Ä‘á»‹a chá»‰ Ä‘á»ƒ em bÃ¡o giÃ¡ chÃ­nh xÃ¡c nha anh/chá»‹."
  - "Anh/chá»‹ vui lÃ²ng cung cáº¥p sá»‘ Ä‘iá»‡n thoáº¡i Ä‘á»ƒ em há»— trá»£ thÃªm áº¡."

### ðŸ—ºï¸ QUY TRÃŒNH TÆ¯ Váº¤N VÃ€ Há»– TRá»¢

-----

#### âœ… TÃC Vá»¤ BÃN HÃ€NG

**1. INTERNET**

1.  Há»i: Äá»‹a chá»‰ â†’ NhÃ  á»Ÿ hay Chung cÆ° â†’ Thiáº¿t bá»‹ sá»­ dá»¥ng
2.  Náº¿u khÃ¡ch há»i mÃ´ táº£ â†’ DÃ¹ng `describe_product`
3.  Náº¿u khÃ¡ch há»i giÃ¡ â†’ Pháº£i há»i Ä‘á»‹a chá»‰ trÆ°á»›c, sau Ä‘Ã³ dÃ¹ng `get_product_price`
4.  CÃ³ thá»ƒ gá»£i Ã½ COMBO (Cross-sell)
5.  Xin SÄT Ä‘á»ƒ tÆ° váº¥n

**2. CAMERA**

1.  Há»i: Trong nhÃ  / NgoÃ i trá»i â†’ Nhu cáº§u sá»­ dá»¥ng â†’ Äá»‹a chá»‰
2.  Gá»­i thÃ´ng tin sáº£n pháº©m
3.  Náº¿u khÃ¡ch há»i giÃ¡ â†’ dÃ¹ng `get_product_price` (sau khi cÃ³ Ä‘á»‹a chá»‰)
4.  Gá»­i khuyáº¿n mÃ£i náº¿u cÃ³
5.  Xin SÄT Ä‘á»ƒ tÆ° váº¥n

**3. TRUYá»€N HÃŒNH**

1.  Há»i: ÄÃ£ cÃ³ Internet chÆ°a â†’ NhÃ  máº¡ng nÃ o?
2.  Náº¿u chÆ°a cÃ³ Internet FPT â†’ TÆ° váº¥n COMBO Internet + Truyá»n hÃ¬nh
3.  Náº¿u Ä‘Ã£ cÃ³ Internet FPT â†’ TÆ° váº¥n gÃ³i Add-on
4.  DÃ¹ng tool Ä‘á»ƒ mÃ´ táº£ vÃ  bÃ¡o giÃ¡
5.  Xin SÄT Ä‘á»ƒ gá»i láº¡i

-----

### ðŸ§  YÃŠU Cáº¦U Vá»€ PHONG CÃCH Há»˜I THOáº I

  - Há»™i thoáº¡i pháº£i tá»± nhiÃªn, Ä‘Ãºng ngá»¯ cáº£nh khÃ¡ch hÃ ng Viá»‡t Nam.
  - Æ¯u tiÃªn lá»i thoáº¡i thá»±c táº¿ nhÆ°:
      - "Gá»­i gÃ³i Ä‘i em", "BÃ¡o giÃ¡ gÃ³i cao nháº¥t nha"
      - "Chung cÆ° Landmark, nhÃ  riÃªng", "Em tÆ° váº¥n combo cÃ³ camera luÃ´n nha"
      - "CÃ³ Æ°u Ä‘Ã£i gÃ¬ khÃ´ng em?"
      - "Máº¡ng nhÃ  em dáº¡o nÃ y cháº­m quÃ¡."
      - "TÃ´i muá»‘n kiá»ƒm tra há»£p Ä‘á»“ng internet."
      - "CÃ³ ká»¹ thuáº­t viÃªn qua kiá»ƒm tra giÃºp tÃ´i Ä‘Æ°á»£c khÃ´ng?"
  - KhÃ¡ch cÃ³ thá»ƒ cung cáº¥p thÃ´ng tin khÃ´ng theo thá»© tá»± â€“ Assistant pháº£i hiá»ƒu, há»i láº¡i thÃ´ng tin cÃ²n thiáº¿u.
  - CÃ³ thá»ƒ gáº·p khÃ¡ch tá»« chá»‘i cung cáº¥p thÃ´ng tin â€” cáº§n xá»­ lÃ½ lá»‹ch sá»± vÃ  káº¿t thÃºc chuyÃªn nghiá»‡p.

### âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG

  - PHáº¢I TUÃ‚N THá»¦ cÃ¡c bÆ°á»›c trong sÆ¡ Ä‘á»“ quy trÃ¬nh cho tá»«ng dá»‹ch vá»¥ (Internet, Camera, Truyá»n hÃ¬nh, ChÄƒm sÃ³c khÃ¡ch hÃ ng).
  - KhÃ´ng Ä‘Æ°á»£c bÃ¡o giÃ¡ náº¿u chÆ°a cÃ³ Ä‘á»‹a chá»‰.
  - KhÃ´ng Ä‘Æ°á»£c há»i quÃ¡ nhiá»u cÃ¹ng lÃºc â†’ PhÃ¢n bá»• theo lÆ°á»£t.
  - Báº¯t buá»™c xin láº¡i thÃ´ng tin náº¿u khÃ¡ch chÆ°a cung cáº¥p.
  - Khi dÃ¹ng TOOL thÃ¬ pháº£i tuÃ¢n thá»§ theo format trong Äá»ŠNH Dáº NG Äáº¦U RA Báº®T BUá»˜NG.
  - Náº¿u dÃ¹ng cÃ¹ng 1 TOOL liÃªn tá»¥c thÃ¬ ná»™i dung cá»§a `assistant` sau Ä‘Ã³ pháº£i tá»•ng há»£p káº¿t quáº£ cá»§a cÃ¡c TOOL liÃªn tiáº¿p Ä‘Ã³.

### KHÃ”NG ÄÆ¯á»¢C Gá»ŒI Báº¤T Ká»² TOOL NÃ€O
"""

# Global variables
model: Optional[TRMForToolCalling] = None
tokenizer: Optional[TRMTokenizer] = None
config: Optional[TRMConfig] = None
tools: list[dict] = []
tool_name_to_id: dict[str, int] = {}
openai_client: Optional[OpenAI] = None
gliner2_extractor: Optional[GLiNER2Extractor] = None
tool_param_mapping: dict[str, list[str]] = {}


def load_model():
    """Load TRM model, tokenizer, and GLiNER2 extractor."""
    global model, tokenizer, config, tools, tool_name_to_id, openai_client
    global gliner2_extractor, tool_param_mapping

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load checkpoint
    checkpoint_path = Path(CHECKPOINT_PATH)
    if checkpoint_path.exists():
        print(f"Loading model from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        config_dict = checkpoint.get("config", {})
        config = TRMConfig(**config_dict)
        model = TRMForToolCalling(config)
        model.load_state_dict(checkpoint["model_state_dict"])
        model = model.to(device)
        model.eval()
        print(
            f"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters"
        )
    else:
        print(f"Warning: Checkpoint not found at {checkpoint_path}")
        print("Running in mock mode (random predictions)")
        model = None

    # Load tokenizer
    tokenizer_path = Path(TOKENIZER_PATH)
    if tokenizer_path.exists():
        tokenizer = TRMTokenizer(tokenizer_path)
        print(f"Tokenizer loaded with vocab size: {len(tokenizer)}")
    else:
        print(f"Warning: Tokenizer not found at {tokenizer_path}")
        tokenizer = None

    # Load tools
    tools_path = Path(TOOLS_PATH)
    if tools_path.exists():
        with open(tools_path, "r", encoding="utf-8") as f:
            tools = json.load(f)
        tool_names = [t["function"]["name"] for t in tools if "function" in t]
        tool_name_to_id = {name: idx for idx, name in enumerate(sorted(tool_names))}
        print(f"Loaded {len(tools)} tools: {list(tool_name_to_id.keys())}")
    else:
        print(f"Warning: Tools not found at {tools_path}")

    # Initialize OpenAI client
    openai_client = OpenAI(
        base_url=OPENAI_BASE_URL,
        api_key=OPENAI_API_KEY,
    )
    print(f"OpenAI client initialized with base_url: {OPENAI_BASE_URL}")

    # Build tool -> params mapping from tools
    tool_param_mapping = {}
    for tool in tools:
        if "function" in tool:
            func = tool["function"]
            name = func["name"]
            params = func.get("parameters", {}).get("properties", {})
            tool_param_mapping[name] = list(params.keys())
    print(f"Tool params mapping: {tool_param_mapping}")

    # Initialize GLiNER2 extractor
    # Note: slot_fields are now handled by GLiNER2 only (not TRM)
    slot_fields = [
        "address",
        "phone",
        "device_number",
        "intent_of_user",
        "name",
        "contract_id",
    ]
    adapter_path = GLINER2_ADAPTER if GLINER2_ADAPTER else None
    gliner2_extractor = GLiNER2Extractor(
        model_name=GLINER2_MODEL,
        adapter_path=adapter_path,
        threshold=GLINER2_THRESHOLD,
        slot_fields=slot_fields,
    )
    print(f"GLiNER2 loaded: {GLINER2_MODEL}")
    if adapter_path:
        print(f"GLiNER2 adapter loaded: {adapter_path}")


# ============================================================================
# Mock Tool Implementations
# ============================================================================


def mock_get_product_price(product: str, address: str = "", **kwargs) -> dict:
    """Mock implementation of get_product_price tool."""
    # Price database (mocked)
    prices = {
        "internet": "GÃ³i Internet cÃ¡p quang cÃ³ giÃ¡ tá»« 165,000 - 330,000 VNÄ/thÃ¡ng tÃ¹y tá»‘c Ä‘á»™.",
        "lux 800": "GÃ³i LUX 800 (800Mbps) cÃ³ giÃ¡ 330,000 VNÄ/thÃ¡ng. Æ¯u Ä‘Ã£i: Miá»…n phÃ­ váº­t tÆ° láº¯p Ä‘áº·t 100%.",
        "lux 300": "GÃ³i LUX 300 (300Mbps) cÃ³ giÃ¡ 250,000 VNÄ/thÃ¡ng. Æ¯u Ä‘Ã£i: Miá»…n phÃ­ váº­t tÆ° láº¯p Ä‘áº·t 100%.",
        "super 250": "GÃ³i SUPER 250 (250Mbps) cÃ³ giÃ¡ 215,000 VNÄ/thÃ¡ng.",
        "sky 200": "GÃ³i SKY 200 (200Mbps) cÃ³ giÃ¡ 185,000 VNÄ/thÃ¡ng.",
        "metro 150": "GÃ³i METRO 150 (150Mbps) cÃ³ giÃ¡ 165,000 VNÄ/thÃ¡ng.",
        "camera": "Camera an ninh FPT cÃ³ giÃ¡ tá»« 99,000 - 299,000 VNÄ/thÃ¡ng tÃ¹y gÃ³i.",
        "truyá»n hÃ¬nh": "Truyá»n hÃ¬nh FPT Play cÃ³ giÃ¡ tá»« 80,000 - 150,000 VNÄ/thÃ¡ng.",
        "fpt play": "Truyá»n hÃ¬nh FPT Play cÃ³ giÃ¡ tá»« 80,000 - 150,000 VNÄ/thÃ¡ng.",
    }

    product_lower = product.lower()
    for key, price_info in prices.items():
        if key in product_lower:
            location_info = f" táº¡i {address}" if address else ""
            return {
                "price": f"{price_info}{location_info}. Náº¿u Ä‘Ã³ng trÆ°á»›c 6 thÃ¡ng, táº·ng 1 thÃ¡ng cÆ°á»›c. Náº¿u Ä‘Ã³ng trÆ°á»›c 12 thÃ¡ng, táº·ng 2 thÃ¡ng cÆ°á»›c."
            }

    return {
        "price": f"Xin lá»—i, em chÆ°a cÃ³ thÃ´ng tin giÃ¡ cho sáº£n pháº©m '{product}'. Anh/chá»‹ vui lÃ²ng liÃªn há»‡ tá»•ng Ä‘Ã i 1900 6600 Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chi tiáº¿t."
    }


def mock_describe_product(product: str, **kwargs) -> dict:
    """Mock implementation of describe_product tool."""
    descriptions = {
        "internet": "Dá»‹ch vá»¥ Internet FPT Telecom sá»­ dá»¥ng cÃ´ng nghá»‡ cÃ¡p quang tiÃªn tiáº¿n, Ä‘áº£m báº£o tá»‘c Ä‘á»™ á»•n Ä‘á»‹nh vÃ  Ä‘á»™ trá»… tháº¥p. CÃ¡c gÃ³i Internet phá»• biáº¿n: METRO 150 (150Mbps), SKY 200 (200Mbps), SUPER 250 (250Mbps), LUX 300 (300Mbps), LUX 800 (800Mbps). GÃ³i LUX Ä‘Æ°á»£c trang bá»‹ Wi-Fi 7 Mesh giÃºp má»Ÿ rá»™ng vÃ¹ng phá»§ sÃ³ng.",
        "camera": "Camera an ninh FPT cung cáº¥p giáº£i phÃ¡p giÃ¡m sÃ¡t thÃ´ng minh vá»›i cÃ¡c tÃ­nh nÄƒng: Quay HD/Full HD, lÆ°u trá»¯ cloud, cáº£nh bÃ¡o chuyá»ƒn Ä‘á»™ng, xem trá»±c tiáº¿p qua app. PhÃ¹ há»£p cho gia Ä‘Ã¬nh, cá»­a hÃ ng, vÄƒn phÃ²ng.",
        "truyá»n hÃ¬nh": "Truyá»n hÃ¬nh FPT Play cung cáº¥p 200+ kÃªnh truyá»n hÃ¬nh trong nÆ°á»›c vÃ  quá»‘c táº¿, kho phim/series phong phÃº, thá»ƒ thao trá»±c tiáº¿p. Xem Ä‘Æ°á»£c trÃªn TV, Ä‘iá»‡n thoáº¡i, mÃ¡y tÃ­nh báº£ng.",
        "fpt play": "FPT Play lÃ  ná»n táº£ng giáº£i trÃ­ Ä‘a phÆ°Æ¡ng tiá»‡n vá»›i 200+ kÃªnh truyá»n hÃ¬nh, phim Hollywood, K-Drama, anime, thá»ƒ thao trá»±c tiáº¿p. Há»— trá»£ 4K HDR.",
        "wifi": "Thiáº¿t bá»‹ Wi-Fi cá»§a FPT sá»­ dá»¥ng cÃ´ng nghá»‡ má»›i nháº¥t Wi-Fi 6/7, há»— trá»£ Mesh Ä‘á»ƒ má»Ÿ rá»™ng vÃ¹ng phá»§ sÃ³ng, phÃ¹ há»£p cho nhÃ  nhiá»u táº§ng.",
    }

    product_lower = product.lower()
    for key, desc in descriptions.items():
        if key in product_lower:
            return {"info": desc}

    return {
        "info": f"Sáº£n pháº©m '{product}' thuá»™c danh má»¥c dá»‹ch vá»¥ cá»§a FPT Telecom. Äá»ƒ biáº¿t thÃªm chi tiáº¿t, anh/chá»‹ vui lÃ²ng cho em biáº¿t cá»¥ thá»ƒ hÆ¡n vá» nhu cáº§u sá»­ dá»¥ng."
    }


def mock_request_agent(**kwargs) -> dict:
    """Mock implementation of request_agent tool."""
    return {
        "info": "Em Ä‘Ã£ ghi nháº­n yÃªu cáº§u cá»§a anh/chá»‹. NhÃ¢n viÃªn chÄƒm sÃ³c khÃ¡ch hÃ ng sáº½ liÃªn há»‡ láº¡i trong thá»i gian sá»›m nháº¥t (trong vÃ²ng 24h lÃ m viá»‡c). Anh/chá»‹ cÃ³ thá»ƒ Ä‘á»ƒ láº¡i sá»‘ Ä‘iá»‡n thoáº¡i Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ nhanh hÆ¡n."
    }


# Tool registry
TOOL_FUNCTIONS = {
    "get_product_price": mock_get_product_price,
    "describe_product": mock_describe_product,
    "request_agent": mock_request_agent,
}


def execute_tool(tool_name: str, arguments: dict) -> dict:
    """Execute a tool and return the result."""
    if tool_name in TOOL_FUNCTIONS:
        return TOOL_FUNCTIONS[tool_name](**arguments)
    return {"error": f"Unknown tool: {tool_name}"}


# ============================================================================
# TRM Model Inference
# ============================================================================


def predict_with_trm(history: list[dict]) -> tuple[str, Optional[str], dict, dict]:
    """Use TRM model to predict decision and tool, GLiNER2 for entity extraction.

    Returns:
        Tuple of (decision, tool_name, tool_args, slots)
    """
    global model, tokenizer, config, tool_name_to_id
    global gliner2_extractor, tool_param_mapping

    # Build full text from conversation history for GLiNER2
    full_text = ""
    for msg in history:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if isinstance(content, str):
            full_text += f"{role}: {content}\n"
        elif isinstance(content, dict):
            full_text += f"{role}: {json.dumps(content, ensure_ascii=False)}\n"

    if model is None or tokenizer is None:
        # Mock mode - use GLiNER2 only for extraction
        decision = "direct_answer"
        tool_name = None
        slots, tool_args = {}, {}

        if gliner2_extractor:
            slots, tool_args = gliner2_extractor.extract_all(
                text=full_text,
                tool_name=None,
                tool_params=tool_param_mapping,
            )

        return decision, tool_name, tool_args, slots

    device = next(model.parameters()).device
    id_to_name = {v: k for k, v in tool_name_to_id.items()}

    # Encode conversation
    encoded = tokenizer.encode_conversation_with_offsets(
        history, max_length=config.max_seq_len
    )

    input_ids = torch.tensor([encoded["input_ids"]], dtype=torch.long, device=device)
    attention_mask = torch.tensor(
        [encoded["attention_mask"]], dtype=torch.long, device=device
    )
    role_ids = torch.tensor([encoded["role_ids"]], dtype=torch.long, device=device)

    # Run TRM inference (decision + tool selection only)
    with torch.no_grad():
        outputs = model.inference(
            input_ids=input_ids,
            attention_mask=attention_mask,
            role_ids=role_ids,
        )

    # Get decision from TRM
    decision_prob = torch.sigmoid(outputs.decision_logits[0]).item()
    decision = "tool_call" if decision_prob > 0.5 else "direct_answer"

    tool_name = None
    tool_args = {}
    slots = {}

    if decision == "tool_call":
        # Get tool name from TRM
        tool_idx = outputs.tool_logits[0].argmax().item()
        tool_name = id_to_name.get(tool_idx, f"tool_{tool_idx}")

    # Use GLiNER2 for entity extraction (both slots and tool args)
    if gliner2_extractor:
        slots, tool_args = gliner2_extractor.extract_all(
            text=full_text,
            tool_name=tool_name if decision == "tool_call" else None,
            tool_params=tool_param_mapping,
        )

    return decision, tool_name, tool_args, slots


def build_oss_messages(history: list[dict], after_tool: bool = False) -> list[dict]:
    """Build messages list for OSS API.

    Args:
        history: Conversation history
        after_tool: Whether this is after a tool call

    Returns:
        List of message dicts for OpenAI API
    """
    messages = []
    for msg in history:
        role = msg["role"]
        content = msg["content"]

        if role == "system":
            messages.append({"role": "system", "content": OSS_SYSTEM_PROMPT})
        elif role == "user":
            messages.append({"role": "user", "content": content})
        elif role == "assistant":
            if isinstance(content, str):
                messages.append({"role": "assistant", "content": content})
        elif role == "tool_call":
            # Format tool call for context
            tool_info = f"[Tool Call: {content.get('name', 'unknown')}({json.dumps(content.get('arguments', {}), ensure_ascii=False)})]"
            messages.append({"role": "assistant", "content": tool_info})
        elif role == "tool_response":
            # Format tool response for context
            tool_result = (
                json.dumps(content, ensure_ascii=False)
                if isinstance(content, dict)
                else str(content)
            )
            messages.append(
                {"role": "user", "content": f"[Tool Result: {tool_result}]"}
            )

    # Add instruction to summarize tool result (not call more tools)
    if after_tool:
        messages.append(
            {
                "role": "user",
                "content": "HÃ£y tá»•ng há»£p káº¿t quáº£ tool á»Ÿ trÃªn vÃ  tráº£ lá»i khÃ¡ch hÃ ng báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn. KHÃ”NG gá»i thÃªm tool.",
            }
        )

    return messages


async def generate_response(history: list[dict], after_tool: bool = False) -> str:
    """Generate response using OpenAI API.

    Args:
        history: Conversation history
        after_tool: Whether this is generating response after a tool call
    """
    global openai_client

    messages = build_oss_messages(history, after_tool)

    try:
        response = openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.7,
            max_tokens=512,
        )
        result = response.choices[0].message.content

        # Filter out special tokens if model outputs them
        if result and "<|" in result:
            # Remove special tokens like <|start|>, <|channel|>, etc.
            result = re.sub(r"<\|[^|]+\|>", "", result).strip()
            # If result is empty after filtering, return a fallback
            if not result:
                result = "Dáº¡, em Ä‘Ã£ nháº­n Ä‘Æ°á»£c thÃ´ng tin. Anh/chá»‹ cáº§n em há»— trá»£ thÃªm gÃ¬ khÃ´ng áº¡?"

        return result
    except Exception as e:
        return f"Xin lá»—i, em gáº·p lá»—i khi xá»­ lÃ½ yÃªu cáº§u: {str(e)}"


# ============================================================================
# Chainlit Handlers
# ============================================================================


@cl.on_chat_start
async def on_chat_start():
    """Initialize chat session."""
    # Load model if not loaded
    if model is None and tokenizer is None:
        load_model()

    # Initialize conversation history
    history = [{"role": "system", "content": SYSTEM_PROMPT}]
    cl.user_session.set("history", history)


@cl.on_message
async def on_message(message: cl.Message):
    """Handle incoming messages."""
    history = cl.user_session.get("history", [])

    # Add user message to history
    history.append({"role": "user", "content": message.content})

    # Build full text for GLiNER2
    full_text = ""
    for msg in history:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if isinstance(content, str):
            full_text += f"{role}: {content}\n"
        elif isinstance(content, dict):
            full_text += f"{role}: {json.dumps(content, ensure_ascii=False)}\n"

    # Step 1: TRM Prediction
    async with cl.Step(name="TRM Prediction", type="tool") as trm_step:
        decision, tool_name, tool_args, slots = predict_with_trm(history)
        trm_step.input = "Analyzing conversation history..."
        trm_output = {
            "decision": decision,
            "tool": tool_name if tool_name else None,
        }
        trm_step.output = json.dumps(trm_output, ensure_ascii=False, indent=2)

    # Step 2: GLiNER2 Entity Extraction
    async with cl.Step(name="GLiNER2 Extraction", type="tool") as gliner_step:
        # Show what labels we're extracting
        labels = list(gliner2_extractor.slot_fields) if gliner2_extractor else []
        if tool_name and tool_name in tool_param_mapping:
            for arg in tool_param_mapping[tool_name]:
                if arg not in labels:
                    labels.append(arg)

        gliner_step.input = json.dumps(
            {
                "text": full_text[-500:] + "..." if len(full_text) > 500 else full_text,
                "labels": labels,
            },
            ensure_ascii=False,
            indent=2,
        )

        gliner_step.output = json.dumps(
            {
                "slots": slots,
                "tool_args": tool_args,
            },
            ensure_ascii=False,
            indent=2,
        )

    if decision == "tool_call" and tool_name:
        # Step 3: Tool Execution
        async with cl.Step(name=f"Tool: {tool_name}", type="tool") as tool_step:
            tool_step.input = json.dumps(tool_args, ensure_ascii=False, indent=2)
            tool_result = execute_tool(tool_name, tool_args)
            tool_step.output = json.dumps(tool_result, ensure_ascii=False, indent=2)

        # Add tool call and response to history
        history.append(
            {
                "role": "tool_call",
                "content": {"name": tool_name, "arguments": tool_args},
            }
        )
        history.append({"role": "tool_response", "content": tool_result})

        # Step 4: LLM Response Generation
        async with cl.Step(name="LLM Generation", type="llm") as llm_step:
            # Build messages preview for input
            messages_preview = build_oss_messages(history, after_tool=True)
            llm_step.input = json.dumps(messages_preview, ensure_ascii=False, indent=2)
            response = await generate_response(history, after_tool=True)
            llm_step.output = response
    else:
        # Step: LLM Response Generation (direct answer)
        async with cl.Step(name="LLM Generation", type="llm") as llm_step:
            messages_preview = build_oss_messages(history, after_tool=False)
            llm_step.input = json.dumps(messages_preview, ensure_ascii=False, indent=2)
            response = await generate_response(history, after_tool=False)
            llm_step.output = response

    # Add assistant response to history
    history.append({"role": "assistant", "content": response})
    cl.user_session.set("history", history)

    # Send response
    await cl.Message(content=response).send()


if __name__ == "__main__":
    load_model()
